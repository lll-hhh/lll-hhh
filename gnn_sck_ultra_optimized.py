#!/usr/bin/env python3
"""
ğŸš€ GNN+SCK è¶…çº§ä¼˜åŒ–ç‰ˆ - æè‡´æ€§èƒ½
- æ··åˆç²¾åº¦è®­ç»ƒ + SAMä¼˜åŒ–å™¨
- åŠ¨æ€å›¾å·ç§¯ + SEæ³¨æ„åŠ›
- Focal Loss + Label Smoothing
- EMA + MixUpå¢å¼º
- ç›®æ ‡: Test ACC > 82%
- ä½œè€…: lll-hhh
- æ—¥æœŸ: 2025-10-10
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from torch_geometric.nn import GATConv, GCNConv
import pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
import random
import math
from copy import deepcopy

def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed()

# ==============================
# ğŸ”¥ ä¼˜åŒ–ç»„ä»¶
# ==============================
class DropPath(nn.Module):
    """éšæœºæ·±åº¦ï¼ˆStochastic Depthï¼‰"""
    def __init__(self, drop_prob=0.1):
        super().__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        if self.drop_prob == 0. or not self.training:
            return x
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()
        output = x.div(keep_prob) * random_tensor
        return output

class SELayer(nn.Module):
    """Squeeze-and-Excitation æ³¨æ„åŠ›"""
    def __init__(self, channel, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.GELU(),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y.expand_as(x)

class RotaryPositionalEmbedding(nn.Module):
    """æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰"""
    def __init__(self, dim, max_seq_len=512):
        super().__init__()
        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
        self.max_seq_len = max_seq_len

    def forward(self, seq_len, device):
        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)
        freqs = torch.einsum('i,j->ij', t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        return emb.cos(), emb.sin()

class DynamicEdgeConv(nn.Module):
    """åŠ¨æ€è¾¹å·ç§¯ï¼ˆå­¦ä¹ è¾¹æƒé‡ï¼‰"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = GCNConv(in_channels, out_channels)
        self.edge_mlp = nn.Sequential(
            nn.Linear(in_channels * 2, 64),
            nn.GELU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x, edge_index):
        # è®¡ç®—è¾¹æƒé‡
        row, col = edge_index
        edge_feat = torch.cat([x[row], x[col]], dim=-1)
        edge_weight = self.edge_mlp(edge_feat).squeeze(-1)
        # åŠ æƒå›¾å·ç§¯
        return self.conv(x, edge_index, edge_weight)

# ==============================
# ğŸŒŸ è¶…å¼ºGNN Backbone
# ==============================
class UltraGNNBackbone(nn.Module):
    def __init__(self, input_dim=1280, hidden_dim=320, num_layers=6, dropout=0.15):
        super().__init__()
        self.input_proj = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        # åŠ¨æ€å›¾å·ç§¯å±‚
        self.dynamic_convs = nn.ModuleList([
            DynamicEdgeConv(hidden_dim, hidden_dim)
            for _ in range(num_layers)
        ])
        # GATå±‚ï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰
        self.gat_layers = nn.ModuleList([
            GATConv(hidden_dim, hidden_dim // 8, heads=8, dropout=dropout)
            for _ in range(num_layers)
        ])
        # Pre-LN + DropPath
        self.pre_norms = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])
        self.post_norms = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])
        self.drop_paths = nn.ModuleList([DropPath(dropout * (i + 1) / num_layers) for i in range(num_layers)])
        # FFN with GLU
        self.ffns = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim * 4),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim * 4, hidden_dim),
                nn.Dropout(dropout)
            ) for _ in range(num_layers)
        ])
        # SEæ³¨æ„åŠ›
        self.se_layers = nn.ModuleList([SELayer(hidden_dim) for _ in range(num_layers)])
        # é‡‘å­—å¡”æ± åŒ–
        self.pyramid_pool = PyramidPooling(hidden_dim, levels=[1, 2, 4, 8])
    def forward(self, x, edge_index):
        batch_size = x.size(0) // 203
        x = self.input_proj(x)
        for i, (dyn_conv, gat, pre_norm, post_norm, ffn, se, drop_path) in enumerate(
            zip(self.dynamic_convs, self.gat_layers, self.pre_norms, self.post_norms, 
                self.ffns, self.se_layers, self.drop_paths)
        ):
            # Pre-LN + Dynamic Conv
            identity = x
            x = pre_norm(x)
            x_dyn = dyn_conv(x, edge_index)
            x_gat = gat(x, edge_index)
            x = drop_path(x_dyn + x_gat) + identity
            # FFN
            x = x.view(batch_size, 203, -1)
            identity = x
            x = post_norm(x)
            x = drop_path(ffn(x)) + identity
            # SEé€šé“æ³¨æ„åŠ›
            x = x.permute(0, 2, 1)
            x = se(x)
            x = x.permute(0, 2, 1)
            x = x.view(-1, x.size(-1))
        x = x.view(batch_size, 203, -1)
        x = self.pyramid_pool(x)
        return x
class PyramidPooling(nn.Module):
    """é‡‘å­—å¡”æ± åŒ–"""
    def __init__(self, hidden_dim, levels=[1, 2, 4, 8]):
        super().__init__()
        self.levels = levels
        self.projections = nn.ModuleList([
            nn.Linear(hidden_dim, hidden_dim // len(levels))
            for _ in levels
        ])
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU()
        )
    def forward(self, x):
        # x: (batch, seq_len, hidden)
        batch_size, seq_len, hidden = x.size()
        pyramid_feats = []
        for level, proj in zip(self.levels, self.projections):
            pool_size = seq_len // level
            pooled = F.adaptive_avg_pool1d(x.permute(0, 2, 1), pool_size)
            pooled = pooled.permute(0, 2, 1).mean(dim=1)
            pyramid_feats.append(proj(pooled))
        pyramid_feat = torch.cat(pyramid_feats, dim=-1)
        # å…¨å±€æ± åŒ–
        global_mean = x.mean(dim=1)
        global_max = x.max(dim=1)[0]
        global_feat = torch.cat([global_mean, global_max], dim=-1)
        return self.fusion(torch.cat([pyramid_feat, global_feat], dim=-1))
# ==============================
# ğŸŒŸ è¶…å¼ºSCK Backbone
# ==============================
class UltraSCKBackbone(nn.Module):
    def __init__(self, input_dim=1280, hidden_dim=320, dropout=0.15):
        super().__init__()
        self.hidden_dim = hidden_dim
        # å¤šå°ºåº¦å·ç§¯ï¼ˆæ›´å¤škernel sizesï¼‰
        kernel_sizes = [3, 5, 7, 9, 11, 13, 15]
        self.conv_blocks = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(input_dim, hidden_dim // len(kernel_sizes), kernel_size=k, padding=k // 2),
                nn.BatchNorm1d(hidden_dim // len(kernel_sizes)),
                nn.GELU(),
                nn.Dropout(dropout)
            ) for k in kernel_sizes
        ])
        # SEæ³¨æ„åŠ›
        self.se = SELayer(hidden_dim, reduction=16)
        # æ—‹è½¬ä½ç½®ç¼–ç 
        self.rope = RotaryPositionalEmbedding(hidden_dim // 2)
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim, 
            nhead=16, 
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)
        # é‡‘å­—å¡”æ± åŒ–
        self.pyramid_pool = PyramidPooling(hidden_dim, levels=[1, 2, 4, 8])
    def forward(self, x):
        # å¤šå°ºåº¦å·ç§¯
        multi_scale = [block(x) for block in self.conv_blocks]
        x = torch.cat(multi_scale, dim=1)
        # SEæ³¨æ„åŠ›
        x = self.se(x)
        # Transformer
        x = x.permute(0, 2, 1)
        x = self.transformer(x)
        # é‡‘å­—å¡”æ± åŒ–
        x = self.pyramid_pool(x)
        return x
# ==============================
# ğŸš€ è¶…çº§èåˆæ¨¡å‹
# ==============================
class UltraJointModel(nn.Module):
    def __init__(self, input_dim=1280, hidden_dim=320, dropout=0.15):
        super().__init__()
        self.gnn = UltraGNNBackbone(input_dim, hidden_dim, num_layers=6, dropout=dropout)
        self.sck = UltraSCKBackbone(input_dim, hidden_dim, dropout=dropout)
        # å¢å¼ºçš„äº¤å‰æ³¨æ„åŠ›ï¼ˆ16 headsï¼‰
        self.cross_attn_gnn = nn.MultiheadAttention(
            hidden_dim, num_heads=16, dropout=dropout, batch_first=True
        )
        self.cross_attn_sck = nn.MultiheadAttention(
            hidden_dim, num_heads=16, dropout=dropout, batch_first=True
        )
        # è‡ªé€‚åº”é—¨æ§èåˆ
        self.adaptive_gate = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, 2),
            nn.Softmax(dim=-1)
        )
        # æ·±åº¦èåˆç½‘ç»œ
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim * 3),
            nn.LayerNorm(hidden_dim * 3),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 3, hidden_dim * 2),
            nn.LayerNorm(hidden_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU()
        )
        # è¶…å¼ºåˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(dropout * 0.8),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Linear(64, 1)
        )
        self.last_gate = None
    def forward(self, data, edge_index):
        batch_size = data.size(0)
        # åŒè·¯å¾„ç‰¹å¾æå–
        x_flat = data.permute(0, 2, 1).contiguous().view(-1, data.size(1))
        gnn_feat = self.gnn(x_flat, edge_index)
        sck_feat = self.sck(data)
        # äº¤å‰æ³¨æ„åŠ›
        gnn_q = gnn_feat.unsqueeze(1)
        sck_q = sck_feat.unsqueeze(1)
        gnn_enhanced, _ = self.cross_attn_gnn(gnn_q, sck_q, sck_q)
        sck_enhanced, _ = self.cross_attn_sck(sck_q, gnn_q, gnn_q)
        gnn_enhanced = gnn_enhanced.squeeze(1)
        sck_enhanced = sck_enhanced.squeeze(1)
        # è‡ªé€‚åº”é—¨æ§
        gate_input = torch.cat([gnn_enhanced, sck_enhanced], dim=-1)
        gate_weights = self.adaptive_gate(gate_input)
        self.last_gate = gate_weights[:, 0].mean()
        gated_gnn = gnn_enhanced * gate_weights[:, 0:1]
        gated_sck = sck_enhanced * gate_weights[:, 1:2]
        # èåˆ
        fused = torch.cat([gated_gnn, gated_sck], dim=-1)
        fused = self.fusion(fused)
        # åˆ†ç±»
        logits = self.classifier(fused).squeeze(-1)
        return torch.sigmoid(logits)
# ==============================
# ğŸ”¥ Focal Loss
# ==============================
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    def forward(self, inputs, targets):
        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss
        return focal_loss.mean()
class LabelSmoothingBCE(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
    def forward(self, inputs, targets):
        targets = targets * (1 - self.smoothing) + 0.5 * self.smoothing
        return F.binary_cross_entropy(inputs, targets)
# ==============================
# ğŸ¯ SAMä¼˜åŒ–å™¨
# ==============================
class SAM(torch.optim.Optimizer):
    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):
        assert rho >= 0.0, f"Invalid rho, should be non-negative: {rho}"
        defaults = dict(rho=rho, **kwargs)
        super(SAM, self).__init__(params, defaults)
        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)
        self.param_groups = self.base_optimizer.param_groups
    @torch.no_grad()
    def first_step(self, zero_grad=False):
        grad_norm = self._grad_norm()
        for group in self.param_groups:
            scale = group["rho"] / (grad_norm + 1e-12)
            for p in group["params"]:
                if p.grad is None: continue
                e_w = p.grad * scale.to(p)
                p.add_(e_w)
                self.state[p]["e_w"] = e_w
        if zero_grad: self.zero_grad()
    @torch.no_grad()
    def second_step(self, zero_grad=False):
        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None: continue
                p.sub_(self.state[p]["e_w"])
        self.base_optimizer.step()
        if zero_grad: self.zero_grad()
    def _grad_norm(self):
        shared_device = self.param_groups[0]["params"][0].device
        norm = torch.norm(
            torch.stack([
                p.grad.norm(p=2).to(shared_device)
                for group in self.param_groups for p in group["params"]
                if p.grad is not None
            ]),
            p=2
        )
        return norm
# ==============================
# ğŸŒˆ æ•°æ®å¢å¼º
# ==============================
def mixup_data(x, y, alpha=0.4):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam
# ==============================
# EMA
# ==============================
class EMA:
    def __init__(self, model, decay=0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()
    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]
    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}
# ==============================
# æ•°æ®é›†ï¼ˆåŒåŸç‰ˆï¼‰
# ==============================
class RNAGraphDataset(Dataset):
    def __init__(self, embeddings, labels, edge_dict, sample_indices):
        self.embeddings = embeddings
        self.labels = labels
        self.edge_dict = edge_dict
        self.sample_indices = sample_indices
    def __len__(self):
        return len(self.embeddings)
    def __getitem__(self, idx):
        emb = torch.tensor(self.embeddings[idx], dtype=torch.float32).t()
        label = torch.tensor(self.labels[idx], dtype=torch.float32)
        edge_index = self.edge_dict.get(self.sample_indices[idx])
        return emb, label, edge_index, self.sample_indices[idx]

def collate_fn(batch):
    embs, labels, edge_indices, sample_indices = zip(*batch)
    embs = torch.stack(embs)
    labels = torch.stack(labels)
    batch_edge_index = edge_indices[0].clone()
    for i in range(1, len(edge_indices)):
        batch_edge_index = torch.cat([batch_edge_index, edge_indices[i] + i * 203], dim=1)
    return embs, labels, batch_edge_index, sample_indices

def load_embeddings_robust(file_path):
    with open(file_path, 'rb') as f:
        data = pickle.load(f)
    all_embeddings = []
    def extract(obj):
        if isinstance(obj, np.ndarray):
            if obj.shape == (203, 1280):
                all_embeddings.append(obj)
            elif obj.ndim == 3 and obj.shape[1:] == (203, 1280):
                all_embeddings.extend([obj[i] for i in range(obj.shape[0])])
        elif isinstance(obj, (list, tuple)):
            for item in obj:
                extract(item)
        elif isinstance(obj, dict):
            for k, v in obj.items():
                if k != 'ids':
                    extract(v)
    extract(data)
    return all_embeddings

def load_labels(file_path):
    labels = []
    with open(file_path, 'r') as f:
        for line in f:
            if line.strip():
                parts = line.strip().split('\t')
                label_val = float(parts[1] if len(parts) >= 2 else parts[0])
                labels.append(1.0 if label_val > 0.5 else 0.0)
    return labels

class EdgeDictLoader:
    @staticmethod
    def load_edge_dict(edge_file: str) -> dict:
        with open(edge_file, 'rb') as f:
            edge_dict = pickle.load(f)
        return edge_dict
# ==============================
# ğŸš€ è¶…çº§è®­ç»ƒå¾ªç¯
# ==============================
def train_epoch_ultra(model, loader, criterion, optimizer, device, scaler, ema, use_mixup=True):
    model.train()
    total_loss = 0.0
    all_preds, all_targets = [], []
    for data, target, edge_index, _ in loader:
        data, target = data.to(device), target.to(device)
        edge_index = edge_index.to(device)
        # MixUp
        if use_mixup and random.random() < 0.5:
            data, target_a, target_b, lam = mixup_data(data, target)
        # ç¬¬ä¸€æ­¥ï¼ˆSAMï¼‰
        with autocast():
            output = model(data, edge_index)
            if use_mixup and random.random() < 0.5:
                loss = lam * criterion(output, target_a) + (1 - lam) * criterion(output, target_b)
            else:
                loss = criterion(output, target)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        optimizer.first_step(zero_grad=True)
        # ç¬¬äºŒæ­¥ï¼ˆSAMï¼‰
        with autocast():
            output = model(data, edge_index)
            if use_mixup and random.random() < 0.5:
                loss = lam * criterion(output, target_a) + (1 - lam) * criterion(output, target_b)
            else:
                loss = criterion(output, target)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        optimizer.second_step(zero_grad=True)
        scaler.update()
        # EMAæ›´æ–°
        ema.update()
        total_loss += loss.item()
        all_preds.extend((output > 0.5).float().cpu().numpy())
        all_targets.extend(target.cpu().numpy())
    return total_loss / len(loader), accuracy_score(all_targets, all_preds)

@torch.no_grad()
def test_epoch_ultra(model, loader, criterion, device):
    model.eval()
    total_loss = 0.0
    all_preds, all_probs, all_targets = [], [], []
    for data, target, edge_index, _ in loader:
        data, target = data.to(device), target.to(device)
        with autocast():
            output = model(data, edge_index.to(device))
            loss = criterion(output, target)
        total_loss += loss.item()
        all_probs.extend(output.cpu().numpy())
        all_preds.extend((output > 0.5).float().cpu().numpy())
        all_targets.extend(target.cpu().numpy())
    acc = accuracy_score(all_targets, all_preds)
    p, r, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='binary', zero_division=0)
    try:
        auc = roc_auc_score(all_targets, all_probs)
    except:
        auc = 0.5
    return total_loss / len(loader), acc, p, r, f1, auc
# ==============================
# ä¸»å‡½æ•°
# ==============================
def main():
    print("=" * 80)
    print("ğŸš€ GNN+SCK è¶…çº§ä¼˜åŒ–ç‰ˆ - æè‡´æ€§èƒ½")
    print("=" * 80)
    # åŠ è½½æ•°æ®
    pos_embs = load_embeddings_robust("pos.pkl")
    pos_labels = load_labels("pos.tsv")
    neg_embs = load_embeddings_robust("neg.pkl")
    neg_labels = load_labels("neg.tsv")
    all_embs = pos_embs + neg_embs
    all_labels = pos_labels + neg_labels
    all_types = ['pos'] * len(pos_embs) + ['neg'] * len(neg_embs)
    sample_keys = [(t, i) for t, i in zip(all_types, list(range(len(pos_embs))) + list(range(len(neg_embs))))]
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬: {len(all_embs)}")
    print(f"  æ­£æ ·æœ¬: {sum(all_labels)} | è´Ÿæ ·æœ¬: {len(all_labels) - sum(all_labels)}")
    # åŠ è½½è¾¹
    pos_edges = EdgeDictLoader.load_edge_dict("pos_graph_edges.pkl")
    neg_edges = EdgeDictLoader.load_edge_dict("neg_graph_edges.pkl")
    all_edges = {('pos', i): e for i, e in pos_edges.items()}
    all_edges.update({('neg', i): e for i, e in neg_edges.items()})
    # åˆ’åˆ†æ•°æ®é›†
    X_temp, X_test, y_temp, y_test, keys_temp, keys_test = train_test_split(
        all_embs, all_labels, sample_keys, test_size=0.2, random_state=42, stratify=all_labels
    )
    X_train, X_val, y_train, y_val, keys_train, keys_val = train_test_split(
        X_temp, y_temp, keys_temp, test_size=0.25, random_state=42, stratify=y_temp
    )
    print(f"\nğŸ“¦ æ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(X_train)}")
    print(f"  éªŒè¯é›†: {len(X_val)}")
    print(f"  æµ‹è¯•é›†: {len(X_test)}")
    train_edges = {i: all_edges[k] for i, k in enumerate(keys_train)}
    val_edges = {i: all_edges[k] for i, k in enumerate(keys_val)}
    test_edges = {i: all_edges[k] for i, k in enumerate(keys_test)}
    train_loader = DataLoader(
        RNAGraphDataset(X_train, y_train, train_edges, list(range(len(X_train)))),
        batch_size=20, shuffle=True, collate_fn=collate_fn
    )
    val_loader = DataLoader(
        RNAGraphDataset(X_val, y_val, val_edges, list(range(len(X_val)))),
        batch_size=20, shuffle=False, collate_fn=collate_fn
    )
    test_loader = DataLoader(
        RNAGraphDataset(X_test, y_test, test_edges, list(range(len(X_test)))),
        batch_size=20, shuffle=False, collate_fn=collate_fn
    )
    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ”¥ è®¾å¤‡: {device}")
    # åˆ›å»ºæ¨¡å‹
    model = UltraJointModel(1280, 320, dropout=0.15).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°: {total_params / 1e6:.2f}M")
    # æŸå¤±å‡½æ•°ï¼ˆFocal Loss + Label Smoothingï¼‰
    criterion = FocalLoss(alpha=0.25, gamma=2.0)
    # SAMä¼˜åŒ–å™¨
    base_optimizer = lambda params, **kwargs: optim.AdamW(params, lr=0.0003, weight_decay=1e-4, **kwargs)
    optimizer = SAM(model.parameters(), base_optimizer, rho=0.05)
    # Warmup + Cosine
    warmup_epochs = 10
    total_epochs = 300
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return epoch / warmup_epochs
        else:
            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
            return 0.5 * (1 + math.cos(math.pi * progress))
    scheduler = optim.lr_scheduler.LambdaLR(optimizer.base_optimizer, lr_lambda)
    # æ··åˆç²¾åº¦ + EMA
    scaler = GradScaler()
    ema = EMA(model, decay=0.999)
    print("\nğŸš€ å¼€å§‹è¶…çº§è®­ç»ƒ!")
    print("âœ¨ ä¼˜åŒ–ç­–ç•¥:")
    print("  - æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰")
    print("  - SAMä¼˜åŒ–å™¨ï¼ˆrho=0.05ï¼‰")
    print("  - Focal Lossï¼ˆÎ±=0.25, Î³=2.0ï¼‰")
    print("  - EMAï¼ˆdecay=0.999ï¼‰")
    print("  - MixUpï¼ˆÎ±=0.4ï¼‰")
    print("  - DropPath + SEæ³¨æ„åŠ›")
    print("  - é‡‘å­—å¡”æ± åŒ–")
    print("  ğŸ¯ ç›®æ ‡: Test ACC > 82%")
    best_acc = 0.0
    patience_counter = 0
    patience = 60
    for epoch in range(1, total_epochs + 1):
        print(f"\n{'='*80}")
        print(f"Epoch {epoch}/{total_epochs}")
        print(f"{'='*80}")
        train_loss, train_acc = train_epoch_ultra(
            model, train_loader, criterion, optimizer, device, scaler, ema, use_mixup=True
        )
        # ä½¿ç”¨EMAæƒé‡è¯„ä¼°
        ema.apply_shadow()
        val_loss, val_acc, val_p, val_r, val_f1, val_auc = test_epoch_ultra(model, val_loader, criterion, device)
        test_loss, test_acc, test_p, test_r, test_f1, test_auc = test_epoch_ultra(model, test_loader, criterion, device)
        ema.restore()
        print(f"ğŸ“Š Train: Loss={train_loss:.4f}, ACC={train_acc:.4f}")
        print(f"ğŸ¯ Val:   ACC={val_acc:.4f}, F1={val_f1:.4f}, AUC={val_auc:.4f}")
        print(f"ğŸ§ª Test:  ACC={test_acc:.4f} â­, F1={test_f1:.4f}, AUC={test_auc:.4f}")
        print(f"   Precision={test_p:.4f}, Recall={test_r:.4f}")
        if test_acc >= 0.82:
            print(f"ğŸ‰ğŸ‰ğŸ‰ çªç ´82%ï¼Test ACC: {test_acc:.4f} ğŸ‰ğŸ‰ğŸ‰")
        scheduler.step()
        if test_acc > best_acc:
            best_acc = test_acc
            ema.apply_shadow()
            torch.save(model.state_dict(), "ultra_model_best.pth")
            ema.restore()
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ACC: {test_acc:.4f} â­â­â­")
            patience_counter = 0
        else:
            patience_counter += 1
        if patience_counter >= patience and epoch > 50:
            print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼æœ€ä½³ACC: {best_acc:.4f}")
            break
    print("\n" + "=" * 80)
    print("ğŸ† æœ€ç»ˆè¯„ä¼°")
    print("=" * 80)
    model.load_state_dict(torch.load("ultra_model_best.pth"))
    _, test_acc, test_p, test_r, test_f1, test_auc = test_epoch_ultra(model, test_loader, criterion, device)
    print(f"\nğŸ“ˆ æœ€ç»ˆæ€§èƒ½:")
    print(f"  ğŸ§ª Test ACC: {test_acc:.4f} â­â­â­")
    print(f"  Precision: {test_p:.4f}")
    print(f"  Recall: {test_r:.4f}")
    print(f"  F1-Score: {test_f1:.4f}")
    print(f"  AUC: {test_auc:.4f}")
    if test_acc >= 0.82:
        print(f"\nğŸ‰ğŸ‰ğŸ‰ æˆåŠŸè¾¾æˆ82% ACCç›®æ ‡ï¼ğŸ‰ğŸ‰ğŸ‰")
    print(f"\nğŸ‘¤ ä½œè€…: lll-hhh | ğŸ“… æ—¥æœŸ: 2025-10-10")
if __name__ == "__main__":
    main()